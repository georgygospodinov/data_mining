{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №2 - Дерево решений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** до 27 марта 2018, 06:00   \n",
    "**Штраф за опоздание:** -2 балла после 06:00 27 марта, -4 балла после 06:00 3 апреля, -6 баллов после 06:00 10 апреля\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла   \n",
    "\n",
    "\n",
    "Присылать ДЗ необходимо в виде ссылки на свой github репозиторий в slack @alkhamush\n",
    "Необходимо в slack создать таск в приватный чат:   \n",
    "/todo Фамилия Имя *ссылка на гитхаб* @alkhamush   \n",
    "Пример:   \n",
    "/todo Ксения Стройкова https://github.com/stroykova/spheremailru/stroykova_hw2.ipynb @alkhamush   \n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Задание 1 (2 баллов)\n",
    "Разберитесь в коде MyDecisionTreeClassifier, который уже частично реализован. В комментариях, где написано \"Что делает этот блок кода?\", ответьте на этот вопрос. Допишите код там, где написано \"Ваш код\". Ваша реализация дерева должна работать по точности не хуже DecisionTreeClassifier из sklearn. Точность проверяется на wine и Speed Dating Data.\n",
    "\n",
    "###### Задание 2 (2 балла)\n",
    "Добиться скорости работы на fit сравнимой со sklearn wine и Speed Dating Data. \n",
    "Для этого используем numpy. \n",
    "\n",
    "###### Задание 3 (2 балла)\n",
    "Продемонстрируйте умение работать с Pipeline на данных Speed Dating Data и DecisionTreeClassifier. Нужно в pipeline произвести все необходимые преобразования данных и в конце обучить модель. Задание реализуйте под пунктом Задание 3 (уже написано ниже)\n",
    "\n",
    "###### Задание 4 (2 балла)\n",
    "Добавьте функционал, который определяет значения feature importance. Выведите 10 главных фичей под пунктом Задание 4 (уже написано ниже) для MyDecisionTreeClassifier и DecisionTreeClassifier так, чтобы сразу были видны выводы и по MyDecisionTreeClassifier, и по DecisionTreeClassifier. Используем данные Speed Dating Data.\n",
    "\n",
    "###### Задание 5 (2 балла)\n",
    "С помощью GridSearchCV или RandomSearchCV подберите наиболее оптимальные параметры для случайного леса (Выберете 2-3 параметра). Используем данные Speed Dating Data. Задание реализуйте под пунктом Задание 5 (уже написано ниже)\n",
    "\n",
    "\n",
    "**Штрафные баллы:**\n",
    "\n",
    "1. Невыполнение PEP8 -1 балл\n",
    "2. Отсутствие фамилии в имени скрипта (скрипт должен называться по аналогии со stroykova_hw2.ipynb) -1 балл\n",
    "3. Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть уже в git'е. В противном случае -1 балл\n",
    "4. При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст. В противном случае -1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "\n",
    "\n",
    "class MyDecisionTreeClassifier:\n",
    "    NON_LEAF_TYPE = 0\n",
    "    LEAF_TYPE = 1\n",
    "\n",
    "    def __init__(self, min_samples_split=2, max_depth=None,\n",
    "                 sufficient_share=1.0, criterion='gini', max_features=None):\n",
    "        self.tree = dict()\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.sufficient_share = sufficient_share\n",
    "        self.num_class = -1\n",
    "        # self.feature_importances_ = None\n",
    "        if criterion == 'gini':\n",
    "            self.G_function = self.__gini\n",
    "            self.classical_gain = self.__classical_gini\n",
    "        elif criterion == 'entropy':\n",
    "            self.G_function = self.__entropy\n",
    "            self.classical_gain = self.__classical_entropy\n",
    "        elif criterion == 'misclass':\n",
    "            self.G_function = self.__misclass\n",
    "            self.classical_gain = self.__classical_misclass\n",
    "        else:\n",
    "            print('invalid criterion name')\n",
    "            raise\n",
    "\n",
    "        if max_features == 'sqrt':\n",
    "            self.get_feature_ids = self.__get_feature_ids_sqrt\n",
    "        elif max_features == 'log2':\n",
    "            self.get_feature_ids = self.__get_feature_ids_log2\n",
    "        elif max_features is None:\n",
    "            self.get_feature_ids = self.__get_feature_ids_N\n",
    "        else:\n",
    "            print('invalid max_features name')\n",
    "            raise\n",
    "\n",
    "    def __classical_gini(self, y):\n",
    "        return 1 - ((np.bincount(y)/y.size)**2).sum()\n",
    "\n",
    "    def __classical_entropy(self, y):\n",
    "        p = np.bincount(y) / y.size + 1e-9\n",
    "        return -(p * np.log2(p)).sum()\n",
    "\n",
    "    def __classical_misclass(self, y):\n",
    "        return 1 - (np.bincount(y) / y.size).max()\n",
    "\n",
    "    def __gini(self, l_c, l_s, r_c, r_s):\n",
    "        l_s = l_s.astype('float')\n",
    "        r_s = r_s.astype('float')\n",
    "        return -((l_c ** 2).sum(axis=1) / np.squeeze(l_s) +\n",
    "                 (r_c ** 2).sum(axis=1) / np.squeeze(r_s))\n",
    "\n",
    "    def __entropy(self, l_c, l_s, r_c, r_s):\n",
    "        return -(l_c * (l_c / l_s + 1e-9) *\n",
    "                 np.log2(l_c / l_s + 1e-9) + r_c * (r_c / r_s + 1e-9)\n",
    "                 * np.log2(r_c / r_s + 1e-9)).sum(axis=1)\n",
    "\n",
    "    def __misclass(self, l_c, l_s, r_c, r_s):\n",
    "        return -(r_c).max(axis=1) - (l_c).max(axis=1)\n",
    "\n",
    "    def __get_feature_ids_sqrt(self, n_feature):\n",
    "        feature_ids = range(n_feature)\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return feature_ids[:int(np.sqrt(n_feature))]\n",
    "\n",
    "    def __get_feature_ids_log2(self, n_feature):\n",
    "        feature_ids = range(n_feature)\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return feature_ids[:int(np.log2(n_feature))]\n",
    "\n",
    "    def __get_feature_ids_N(self, n_feature):\n",
    "        return np.arange(n_feature)\n",
    "\n",
    "    def __sort_samples(self, x, y):\n",
    "        sorted_idx = x.argsort()\n",
    "        return x[sorted_idx], y[sorted_idx]\n",
    "\n",
    "    def __div_samples(self, X, y, feature_id, threshold):\n",
    "        # left_mask = x[:, feature_id] > threshold\n",
    "        # right_mask = ~left_mask\n",
    "        # return x[left_mask], x[right_mask], y[left_mask], y[right_mask]\n",
    "        left_mask = X[:, feature_id] > threshold\n",
    "        equal_mask = X[:, feature_id] == threshold\n",
    "        right_mask = X[:, feature_id] < threshold\n",
    "        X_l = X[left_mask]\n",
    "        X_r = X[right_mask]\n",
    "        y_l = y[left_mask]\n",
    "        y_r = y[right_mask]\n",
    "        if y_r.size == 0 and y_l.size == 0:\n",
    "            X_l = X[:np.int(X.shape[0] / 2)]\n",
    "            y_l = y[:np.int(X.shape[0] / 2)]\n",
    "            X_r = X[-np.int(X.shape[0] / 2):]\n",
    "            y_r = y[-np.int(X.shape[0] / 2):]\n",
    "            return X_l, X_r, y_l, y_r\n",
    "        if y_r.size == 0:\n",
    "            X_r = X[equal_mask]\n",
    "            y_r = y[equal_mask]\n",
    "            return X_l, X_r, y_l, y_r\n",
    "        if y_l.size == 0:\n",
    "            X_l = X[equal_mask]\n",
    "            y_l = y[equal_mask]\n",
    "            return X_l, X_r, y_l, y_r\n",
    "        X_r = X[right_mask | equal_mask]\n",
    "        y_r = y[right_mask | equal_mask]\n",
    "        return X_l, X_r, y_l, y_r\n",
    "\n",
    "    def __find_threshold(self, x, y):\n",
    "        # функция find_threshold нужна для поиска оптимальногo\n",
    "        # признака, по которому произойдет разбиение:\n",
    "        # принимает столбец определенного признака и столбец целевой переменной\n",
    "        # возвращает impurity(нужно минимизировать) и threshold\n",
    "        # (по нему нужно разбить объекты на 2 класса)\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        # sort_samples y по ключу х\n",
    "        # число различных классов записывается в переменную class_number\n",
    "        sorted_x, sorted_y = self.__sort_samples(x, y)\n",
    "        # class_number = np.unique(y).size\n",
    "\n",
    "        # допустим, что np.unique выдаст [1]\n",
    "        # тогда num_classes -> 1\n",
    "        # при использовании one hot encoding далее при попытке\n",
    "        # обращенияпо индексу sorted_y[r_border_ids - 1] -> 1\n",
    "        # алгоритм выйдет за границу массива. чтобы этого не произошло,\n",
    "        # следует использовать другое определение:\n",
    "        class_number = y.max() + 1\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        # выделяем центральную часть y_sorted (отсекаем по\n",
    "        # min_samples_split слева и справа,\n",
    "        # так как их нельзя делить)\n",
    "        # и записываем результат в splitted_sorted_y\n",
    "        # конструкция внутри np.where сравнивает сдвинутые\n",
    "        # на 1 значения массива splitted_sorted_y\n",
    "        # [x * * * * *]\n",
    "        #   [* * * * * x]\n",
    "        # таким образом, мы получим индексы массива splitted_sorted_y,\n",
    "        # после которых следует элемент другого класса\n",
    "        # для получения индекса элемента, на котором меняется класс,\n",
    "        # прибавляем 1, чтобы вернуться к адресации\n",
    "        # в индексах массива sorted_y, прибавляем min_samples_split\n",
    "        cut_size = self.min_samples_split // 2 - 1\n",
    "        if cut_size == 0:\n",
    "            splitted_sorted_y = sorted_y\n",
    "        else:\n",
    "            splitted_sorted_y = sorted_y[cut_size:-cut_size]\n",
    "        r_border_ids = np.where(splitted_sorted_y[:-1]\n",
    "                                != splitted_sorted_y[1:])[0] + (cut_size + 1)\n",
    "\n",
    "        # r_borders_ids содержит индексы, начиная с\n",
    "        # которых в sorted_y идут объекты другого класса\n",
    "\n",
    "        # если получили, что в данной выборке\n",
    "        # все элементы одного класса, разбивать не надо\n",
    "        if len(r_border_ids) == 0:\n",
    "            return float('+inf'), None\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        # eq_el_count - количество элементов одного класса,\n",
    "        # идущих подряд (вектор из размеров кластеров)\n",
    "        # len(eq_el_count) - количество кластеров в sorted_y\n",
    "        eq_el_count = r_border_ids - np.append([cut_size], r_border_ids[:-1])\n",
    "        # создаем матрицу one_hot_code. число строк: количество кластеров,\n",
    "        # число столбцов: количество классов\n",
    "        one_hot_code = np.zeros((r_border_ids.shape[0], class_number))\n",
    "        # ставим единицу в том столбце класса,\n",
    "        # к которому относится данных кластер\n",
    "        one_hot_code[np.arange(r_border_ids.shape[0]),\n",
    "                     sorted_y[r_border_ids - 1]] = 1\n",
    "        # домножаем получившуюся матрицу посимвольно на столбец,\n",
    "        # содержащий размер кластеров\n",
    "        # таким образом, class_increments - это матрица.\n",
    "        # каждая ее строка - это отдельный кластер,\n",
    "        # индекс ненулевого столбца показывает, к какому\n",
    "        # классу относится данный кластер, а значение - размер кластера\n",
    "        class_increments = one_hot_code * eq_el_count.reshape(-1, 1)\n",
    "        # мы отрезали cut_size элементов, так как не может делить\n",
    "        # в той области, но они есть, их надо учитывать в матрице\n",
    "        class_increments[0] = class_increments[0] + np.bincount(\n",
    "            sorted_y[:cut_size], minlength=class_number)\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        # считаем кумулятивную сумму по столбцам матрицы\n",
    "        # i-я строка показывает, сколько элементов каждого класса\n",
    "        # встретилость от i = 0 до текущего\n",
    "        # np.bincount(sorted_y) содержит общую статистику\n",
    "        # входного столбца целевой переменной\n",
    "        # l_class_count[i] показывает, сколько элементов каждого\n",
    "        # класса будет в левой ноде, если мы сделаем разбиение на индексе i\n",
    "        # тогда в правую ноду попадет все - l_class_count[i]\n",
    "        l_class_count = np.cumsum(class_increments, axis=0)\n",
    "        r_class_count = np.bincount(sorted_y) - l_class_count\n",
    "        # l_sizes[i] - число элементов в левой ноде,\n",
    "        # если разбивать по элементу с индексом i\n",
    "        # соответственно r_sizes = все - l_sizes\n",
    "        l_sizes = r_border_ids.reshape(l_class_count.shape[0], 1)\n",
    "        r_sizes = sorted_y.shape[0] - l_sizes\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        # вычисляем impurity для каждого из таких разбиений\n",
    "        # локальная оптимизация: impurity должна быть минимальной\n",
    "        gs = self.G_function(l_class_count, l_sizes, r_class_count, r_sizes)\n",
    "        idx = np.argmin(gs)\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        # выбираем id элемента, по которому будем бить (он попадет в правую\n",
    "        # ноду). так как l_sizes содержит в себе кумулятивно накопленные\n",
    "        # размеры кластеров, то l_sizrd[idx] как раз будет показывать\n",
    "        # сколько элементов в левой ноде, а сколько в правой\n",
    "        left_el_id = l_sizes[idx][0]\n",
    "        return gs[idx], (sorted_x[left_el_id-1] + sorted_x[left_el_id]) / 2.0\n",
    "\n",
    "    def __make_leaf(self, node_id, y):\n",
    "        prediction = np.bincount(y).argmax()\n",
    "        probs = np.bincount(y) / y.size\n",
    "        self.tree[node_id] = (self.LEAF_TYPE, prediction, probs)\n",
    "\n",
    "    def __fit_node(self, X, y, node_id, depth, pred_f=-1):\n",
    "        # Ваш код\n",
    "        # Необходимо использовать следующее:\n",
    "        # self.LEAF_TYPE\n",
    "        # self.NON_LEAF_TYPE\n",
    "\n",
    "        # self.tree\n",
    "        # self.max_depth\n",
    "        # self.sufficient_share\n",
    "        # self.min_samples_split\n",
    "\n",
    "        # self.get_feature_ids\n",
    "        # self.__find_threshold\n",
    "        # self.__div_samples\n",
    "        # self.__fit_node\n",
    "\n",
    "        if (depth ==\n",
    "            self.max_depth) or (np.bincount(y).max() / y.size\n",
    "                                > self.sufficient_share):\n",
    "            self.__make_leaf(node_id, y)\n",
    "            return\n",
    "        f_id = self.get_feature_ids(X.shape[1])\n",
    "        thresholds = np.array([self.__find_threshold(X[:, id], y)\n",
    "                               for id in f_id])\n",
    "        best_id = thresholds[:, 0].argmin()\n",
    "        _, threshold = thresholds[best_id]\n",
    "        if threshold is None:\n",
    "            self.__make_leaf(node_id, y)\n",
    "            return\n",
    "        X_l, X_r, y_l, y_r = self.__div_samples(X, y, f_id[best_id], threshold)\n",
    "        self.tree[node_id] = (self.NON_LEAF_TYPE, f_id[best_id], threshold)\n",
    "        self.feature_importances_[best_id] += self.classical_gain(\n",
    "            y) - (y_l.size * self.classical_gain(y_l) +\n",
    "                  y_r.size * self.classical_gain(y_r)) / y.size\n",
    "        self.__fit_node(X_l, y_l, 2 * node_id + 1, depth + 1, pred_f)\n",
    "        self.__fit_node(X_r, y_r, 2 * node_id + 2, depth + 1, pred_f)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.num_class = np.unique(y).size\n",
    "        self.feature_importances_ = np.zeros(X.shape[1])\n",
    "        self.__fit_node(X, y, 0, 0)\n",
    "        self.feature_importances_ /= y.size\n",
    "\n",
    "    def __predict_class(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_class(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_class(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[1]\n",
    "\n",
    "    def __predict_probs(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_probs(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_probs(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[2]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.__predict_class(x, 0) for x in X])\n",
    "\n",
    "    def predict_probs(self, X):\n",
    "        return np.array([self.__predict_probs(x, 0) for x in X])\n",
    "\n",
    "    def fit_predict(self, x_train, y_train, predicted_x):\n",
    "        self.fit(x_train, y_train)\n",
    "        return self.predict(predicted_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.13 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24 ms\n"
     ]
    }
   ],
   "source": [
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77661227661227661"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78354978354978355"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# тут делаете то же самое, что и на семинаре https://github.com/stroykova/spheremailru/blob/master/2018-02/lecture_04_trees/pract-speed-dating-trees-proc.ipynb\n",
    "df = pd.read_csv('Speed Dating Data.csv', encoding='cp1251')\n",
    "df = df.iloc[:, :97]\n",
    "df = df.drop(['id'], axis=1)\n",
    "df = df.drop(['idg'], axis=1)\n",
    "df = df.drop(['condtn'], axis=1)\n",
    "df = df.drop(['round'], axis=1)\n",
    "df = df.drop(['position', 'positin1'], axis=1)\n",
    "df = df.drop(['order'], axis=1)\n",
    "df = df.drop(['partner'], axis=1)\n",
    "df = df.drop(['age_o', 'race_o', 'pf_o_att', \n",
    "              'pf_o_sin', 'pf_o_int',\n",
    "              'pf_o_fun', 'pf_o_amb', 'pf_o_sha',\n",
    "              'dec_o', 'attr_o', 'sinc_o', 'intel_o', 'fun_o',\n",
    "              'amb_o', 'shar_o', 'like_o', 'prob_o','met_o'], \n",
    "             axis=1)\n",
    "df.drop_duplicates('iid').age.isnull().sum()\n",
    "df = df.dropna(subset=['age'])\n",
    "df.loc[:, 'field_cd'] = df.loc[:, 'field_cd'].fillna(0)\n",
    "df = df.drop(['field'], axis=1)\n",
    "pd.get_dummies(df, columns=['field_cd'], \n",
    "                prefix='field_cd', prefix_sep='=')\n",
    "\n",
    "df = df.drop(['undergra'], axis=1)\n",
    "df.loc[:, 'mn_sat'] = df.loc[:, 'mn_sat'].str.replace(',', '').astype(np.float)\n",
    "df.loc[:, 'mn_sat'] = df.mn_sat.fillna(-999)\n",
    "df.loc[:, 'tuition'] = df.loc[:, 'tuition'].str.replace(',', '').astype(np.float)\n",
    "df.loc[:, 'tuition'] = df.tuition.fillna(-999)\n",
    "df = df.dropna(subset=['imprelig', 'imprace'])\n",
    "df = df.drop(['from', 'zipcode'], axis=1)\n",
    "df.loc[:, 'income'] = df.loc[:, 'income'].str.replace(',', '').astype(np.float)\n",
    "df.loc[:, 'income'] = df.loc[:, 'income'].fillna(-999)\n",
    "df = df.dropna(subset=['date'])\n",
    "df.loc[:, 'career_c'] = df.loc[:, 'career_c'].fillna(0)\n",
    "df = df.drop(['career'], axis=1)\n",
    "df = df.drop(['sports','tvsports','exercise','dining','museums','art','hiking','gaming',\n",
    "       'clubbing','reading','tv','theater','movies','concerts','music','shopping','yoga'], axis=1)\n",
    "df = df.drop(['expnum'], axis=1)\n",
    "feat = ['iid', 'wave', 'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']\n",
    "temp = df.drop_duplicates(subset=['iid', 'wave']).loc[:, feat]\n",
    "temp.loc[:, 'totalsum'] = temp.iloc[:, 2:].sum(axis=1)\n",
    "df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']].sum(axis=1)\n",
    "df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']] = \\\n",
    "(df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']].T/df.loc[:, 'temp_totalsum'].T).T * 100\n",
    "feat = ['iid', 'wave', 'attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']\n",
    "temp = df.drop_duplicates(subset=['iid', 'wave']).loc[:, feat]\n",
    "temp.loc[:, 'totalsum'] = temp.iloc[:, 2:].sum(axis=1)\n",
    "df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']].sum(axis=1)\n",
    "df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']] = \\\n",
    "(df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']].T/df.loc[:, 'temp_totalsum'].T).T * 100\n",
    "df = df.drop(['temp_totalsum'], axis=1)\n",
    "for i in [4, 5]:\n",
    "    feat = ['attr{}_1'.format(i), 'sinc{}_1'.format(i), \n",
    "            'intel{}_1'.format(i), 'fun{}_1'.format(i), \n",
    "            'amb{}_1'.format(i), 'shar{}_1'.format(i)]\n",
    "    \n",
    "    if i != 4:\n",
    "        feat.remove('shar{}_1'.format(i))\n",
    "    \n",
    "    df = df.drop(feat, axis=1)\n",
    "df = df.drop(['wave'], axis=1)\n",
    "df_male = df.query('gender == 1').drop_duplicates(subset=['iid', 'pid'])\\\n",
    "                                 .drop(['gender'], axis=1)\\\n",
    "                                 .dropna()\n",
    "df_female = df.query('gender == 0').drop_duplicates(subset=['iid'])\\\n",
    "                                   .drop(['gender', 'match', 'int_corr', 'samerace'], axis=1)\\\n",
    "                                   .dropna()\n",
    "        \n",
    "df_female.columns = df_female.columns + '_f'\n",
    "df_female = df_female.drop(['pid_f'], axis=1)\n",
    "df_pair = df_male.join(df_female.set_index('iid_f'), \n",
    "                       on='pid', \n",
    "                       how='inner')\n",
    "df_pair = df_pair.drop(['iid', 'pid'], axis=1)\n",
    "X = df_pair.iloc[:, 1:].values\n",
    "y = df_pair.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)\n",
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 141 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тут должен быть код типа \n",
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.73 s\n"
     ]
    }
   ],
   "source": [
    "# тут должен быть код типа\n",
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества работы на Speed Dating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59376554468579013"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тут должен быть код типа \n",
    "f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5937908022631655"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тут должен быть код типа \n",
    "f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57525335312735182"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('clf', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "f1_score(y_pred=pipeline.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " my_clf \n",
      "\n",
      "| feature    |   importance |\n",
      "|------------+--------------|\n",
      "| int_corr   |  0.0171645   |\n",
      "| samerace   |  0.00492841  |\n",
      "| age        |  0.00180618  |\n",
      "| field_cd_f |  0.0012095   |\n",
      "| field_cd   |  0.00113858  |\n",
      "| age_f      |  0.000912726 |\n",
      "| imprace_f  |  0.000636512 |\n",
      "| mn_sat     |  0.000562853 |\n",
      "| income_f   |  0.000466786 |\n",
      "| shar1_1_f  |  0.000435938 |\n",
      "\n",
      " sklearn \n",
      "\n",
      "| feature    |   importance |\n",
      "|------------+--------------|\n",
      "| int_corr   |    0.082184  |\n",
      "| amb1_1_f   |    0.0326569 |\n",
      "| age_f      |    0.0292627 |\n",
      "| exphappy_f |    0.0277691 |\n",
      "| fun1_1_f   |    0.027369  |\n",
      "| income_f   |    0.025696  |\n",
      "| date_f     |    0.0242891 |\n",
      "| fun2_1_f   |    0.02369   |\n",
      "| amb1_1     |    0.023001  |\n",
      "| tuition_f  |    0.0228923 |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def make_stat(names, clf, n, title):\n",
    "    importances = clf.feature_importances_\n",
    "    top_n = importances.argsort()[-n:][::-1]\n",
    "    table = np.array([names[top_n], importances[top_n]])\n",
    "    print('\\n', title, '\\n')\n",
    "    print(tabulate(table.T, headers=['feature', 'importance'], tablefmt='orgtbl'))\n",
    "    \n",
    "names = df_pair.columns[1:]\n",
    "make_stat(names, my_clf, 10, 'my_clf')\n",
    "make_stat(names, clf, 10, 'sklearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 67, 'min_samples_split': 10, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "param_dist = {\"min_samples_split\": range(2, 11),\n",
    "              \"n_estimators\": range(1, 101),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "n_iter_search = 50\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "random_search.fit(X, y)\n",
    "print(random_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
